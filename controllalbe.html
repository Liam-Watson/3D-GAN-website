<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>PCE-GAN</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<script async="" src="https://unpkg.com/es-module-shims@1.3.6/dist/es-module-shims.js"></script>
	<script type="importmap">
			{
				"imports": {
					"three": "./threejs/build/three.module.js",
					"three/addons/": "./jsm/"
				}
			}
		</script>
		<script type="module" src="./controllableFaces.js"></script>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">PCE-GAN</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="generic.html" class="active">Controllable GAN</a></li>
						<li><a href="conditional.html">Conditional GAN</a></li>
						<li><a href="progressive.html">Progressive GAN</a></li>
					</ul>
				</nav>
			</header>
			
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							
							<h1 class="major">Controllable Point Cloud Expression GAN</h1>
							<h2>Overview</h2>
							<p>Controllable GANs are an extension to the vanilla Generative Adversarial Network architecture which enables 
								unsupervised feature controlled content generation. The normative structure leverages the knowledge embedded in a
								pretrained classifier to control the features of generated output. The classifier is used to manipulate the
								latent noise vector to move in the direction of a desired feature within the generators input latent space. In the case of
								PCE-GAN we employ a PointNet classifier trained on various point cloud facial expressions and a modified DCGAN 
								to generate point cloud faces.  
							</p>
							<hr>
							<h2>Modified DCGAN</h2>
							<p>Deep Convolutional GANs leverage the modelling power granted by convolutional layers. Typically convolutional 
								are used in the context of 2D image generation. However, a similar architecture can be adapted to 3D point cloud
								data. The modified DCGAN architecture is shown below. The model consists of 5 one dimensional convolutional layers for the generator 
								and seven one dimensional convolutional layers for the discriminator. The 1D convolutions are well suited for point cloud data as they
								are able to treat the list of (x,y,z) coordinates as a 1D signal with 3 channels - analogous to color channels in a 2D image.  

							</p>
							<span class="image fit"><img src="images/DCGAN_PC.drawio.png" alt="" /></span>
							<h2>PCE-GAN</h2>	
							<p>
								The PCE-GAN architecture is shown below. The model consists of a modified DCGAN generator and discriminator, a pretrained PointNet classifier
								and a modified latent noise vector. The latent noise vector is modified by the classifier to move in the direction of a desired feature. 
								The classifier is used to generate a feature vector which is then used to modify the latent noise vector. The modified latent noise vector is then
								used as input to the generator to generate a point cloud face.
							</p>
							<span class="image fit"><img src="images/pceGANarch3.png" alt="" /></span>
							<hr>
							<h2>Results</h2>
							<p>
								It is important for this work to consider both quantitative and qualitative results. 
								Quantitative results are important to remove bias and have an objective comparison measure for future works. 
								However qualitative results are vital as humans are especially sensitive to subtle changes in facial expressions
								and as such are able to provide a more practical evaluation of the models performance.
							</p>
							<h3>Quantitative Results</h3>
							<p>The below table shows the quantitative metrics for results generated before adding any latent vector control.
								Included in these results are two comparison papers namely CoMA autoencoder and MeshGAN. 
							</p>
							<div class="table-wrapper">
								<div class="alt">
									<table>
										<thead>
											<tr>
												<th>Method</th>
												<th>Generalization (mm)</th>
												<th>Specificity (mm)</th>
												<th>FID</th>
											</tr>
											
										</thead>
										<tbody>
											<tr>
												<td>CoMA</td>
												<td><b>0.442 ± 0.116</b></td>
												<td>1.60 ± 0.22</td>
												<td>14.24</td>
											</tr>
											<tr>
												<td>MeshGAN</td>
												<td>0.465 ± 0.189</td>
												<td>1.433 ± 0.14 </td>
												<td><b>10.82</b></td>
											</tr>
											<tr>
												<td>PCE-GAN</td>
												<td>0.747 ± 0.0163</td>
												<td><b>0.809 ± 0.0334</b></td>
												<td>13.27</td>
											</tr>
											
										</tbody>
									</table>
									
								</div>
								
							</div>
							<p>The bellow table displays the quantitative metrics for various expressions that PCE-GAN can generate. These results
								show that there is some variation in the generated point clouds. However, all metrics are still within the range 
								of acceptability. The best performing facial expression is the mouth open expression which is likely due to
								the large structural changes that occur in the mouth region when the mouth is opened differentiating such expressions 
								from other expressions more easily.
							</p>
							<div class="table-wrapper">
								<div class="alt">
									<table>
										<thead>
											<tr>
												<th>Expression</th>
												<th>Generalization (mm)</th>
												<th>Specificity (mm)</th>
												<th>KID</th>
												<th>FID</th>
											</tr>
											
										</thead>
										<tbody>
											<tr>
												<td>Bare teeth</td>
												
												<td>1.161 ± 0.157</td>
												<td>0.8788 ± 0.2698</td>
												<td>0.87</td>
												<td>16.59</td>
											</tr>
											<tr>
												<td>Cheeks in</td>
												
												<td>1.329 ± 0.006</td>
												<td>1.162 ± 0.384</td>
												<td>0.63</td>
												<td>15.48</td>
											</tr>
											<tr>
												<td>Mouth Open</td>
												
												<td><b>0.957 ± 0.148 </b></td>
												<td><b>0.7610 ± 0.1662</b></td>
												<td><b>0.32</b></td>
												<td><b>13.03</b></td>
											</tr>
											<tr>
												<td>High Smile</td>
												
												<td>0.7771 ± 0.0198 </td>
												<td>1.032 ± 0.388</td>
												<td>0.59</td>
												<td>15.05 </td>
											</tr>
											
										</tbody>
									</table>
									
								</div>
								
							</div>
							
							<h3>Qualitative</h3>
							<p>The figures bellow show the control process where successive frames are each an independent generation of a new face 
								after latent vector manipulation informed by the classifiers gradients conditioned on the previous face. 
								On the right of the faces one can see the classifiers predicted probability of each target expression (left cheeks in and right
								mouth open). One can see that successive generations are temporally and physically valid which allows intermediate generations
								to be used as animation keyframes for facial expression changes. 
							</p>
							<div class="row">
							<span class="image"><img src="images/output.gif" alt="" /></span>
							<span class="image"><img src="images/output4.gif" alt="" /></span>
							<span class="image"><img src="images/output2.gif" alt="" /></span>
							<span class="image"><img src="images/output3.gif" alt="" /></span>
							</div>
							<h4>Live demo</h4>
							<p>Bellow one can see a live render of some of the final generated results of PCE-GAN. The interactive demonstration 
								contains 10 faces - first the triangle mesh obtained by Poisson surface reconstruction followed by the point cloud 
								that PCE-GAN generates (used in the prior surface reconstruction).	The first displayed face is one created by manually 
								adjusting surface reconstruction parameters to produce much better shading, topology and overall appearance (eyes are replaced by spheres) - 
								this is done to demonstrate the practical use of PCE-GAN as an expert would be able to quickly generate such results.
							</p>
							<h4>Controls:</h4>
							<ul>
								
								<li>Mouse left click and drag to rotate.</li>
								<li> SHIFT + Scroll wheel to zoom. </li>
								<li>N or n to get the next result.</li>
							</ul> 
							
							
							<!-- <span class="image"><img src="images/controlGANface.gif" alt="" /></span>
							<span class="image"><img src="images/controlGANface.gif" alt="" /></span> -->
							<!-- <span class="image"><img src="images/controlGANface.gif" alt="" /></span> -->
							<div id="threejs" style="position:relative; width: 100%; height: 100%;"></div>
							<h2>Future Work</h2>
							<h4>Surface Reconstruction</h4>
							Reconstruction of a mesh from the point set has proven to be a
							large flaw of the point set generation method. In further works,
							investigations into including point set normals in the training and
							generated results. This inclusion would enable both more detailed
							generation and accurate surface reconstruction.
							<h4>GAN Improvements</h4>
							The current formulation of PCE-GAN gives rise to spurious points
							in the generated point set. Further investigations into more advanced GAN architectures such as WGAN or WGAN-GP architectures may reduce the noisy results as such techniques have been
							proven to outperform DCGANs in most applications.
							<h4> Training Data</h4>
							For future work where detailed identity is important, higher fidelity
							training data is recommended as down sampling to 5023 points
							reduces the identity information dramatically while retaining the
							expression information. In order to increase the likelihood of high
							frequency detail (such as wrinkles) being included in generated
							results a higher number of training points is required. A larger
							training set will likely improve the quality of synthetic point sets,
							enabling a smaller learning rate - this does require more computational and memory resources. When extending the dataset one
							should include an increased variety of identities to broaden the
							controllable GANs ability to generate unseen faces. Additionally,
							the inclusion of expression to expression time series data would
							increase the models ability to produce temporally valid expressions
							and enable the model to better impose control over a variety of
							initial starting expressions.
							<h4> Larger Degree of Feature Control </h4>
							Future work may choose to increase the dimension of feature control from expression to identity features. Such an investigation
							would require a classifier trained on detailed face property labelling
							such as the size of facial expressions, age, gender, etc. Training data
							for such a model may be obtained by means of facial scans or use
							of manually constructed data that is often used in content creation.
							<h4> Data Representations </h4>
							It has not been proven that point set data is the most suitable representation for face generation. Further investigations may find
							an encoding of point set data more easily learnable by a GAN
							such as different coordinate systems which may emphasise relative
							point densities, embeddings of points into a space that emphasises
							a specific attribute of the data or a voxel approach to emphasise
							volumetric consistency. Different data representations my additionally aid surface reconstruction with volumetric information or
							a reduction of spurious points due to a higher degree of spacial
							accuracy.
						</div>

					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>